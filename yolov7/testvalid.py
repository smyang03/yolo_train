import argparse
import json
import os
import shutil
import random
import logging
import pickle
from pathlib import Path
from threading import Thread
from collections import defaultdict

import numpy as np
import torch
import yaml
import cv2
from tqdm import tqdm

from models.experimental import attempt_load
from utils.datasets import create_dataloader
from utils.general import (coco80_to_coco91_class, check_dataset, check_file, check_img_size, 
                          check_requirements, box_iou, non_max_suppression, scale_coords, 
                          xyxy2xywh, xywh2xyxy, set_logging, increment_path, colorstr)
from utils.metrics import ap_per_class, ConfusionMatrix
from utils.plots import plot_images, output_to_target, plot_study_txt, plot_one_box
from utils.torch_utils import select_device, time_synchronized, TracedModel

# =============================================================================
# 1. ì¤‘ë³µ ë¼ë²¨ ì œê±° í•¨ìˆ˜ ì¶”ê°€ (ë¼ì¸ ~80 ê·¼ì²˜, safe_class_conversion í•¨ìˆ˜ ë’¤ì— ì¶”ê°€)
# =============================================================================

def remove_duplicate_labels(gt_boxes, tolerance=0.01):
    """
    ì¤‘ë³µ GT ë¼ë²¨ ì œê±° í•¨ìˆ˜
    
    Args:
        gt_boxes: GT ë¼ë²¨ ë¦¬ìŠ¤íŠ¸ [[cls, x, y, w, h], ...]
        tolerance: ì¤‘ë³µ íŒì • í—ˆìš© ì˜¤ì°¨ (normalized coordinates)
    
    Returns:
        unique_boxes: ì¤‘ë³µ ì œê±°ëœ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
        duplicate_info: ì¤‘ë³µ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    if not gt_boxes:
        return [], {'removed_count': 0, 'duplicate_pairs': []}
    
    unique_boxes = []
    duplicate_pairs = []
    removed_indices = set()
    
    for i, box1 in enumerate(gt_boxes):
        if i in removed_indices:
            continue
            
        for j, box2 in enumerate(gt_boxes[i+1:], i+1):
            if j in removed_indices:
                continue
                
            try:
                # ê°™ì€ í´ë˜ìŠ¤ì¸ì§€ í™•ì¸
                cls1 = safe_class_conversion(box1[0])
                cls2 = safe_class_conversion(box2[0])
                
                if cls1 == cls2 and len(box1) >= 5 and len(box2) >= 5:
                    # ìœ„ì¹˜ ì°¨ì´ ê³„ì‚° (normalized coordinates)
                    x_diff = abs(float(box1[1]) - float(box2[1]))
                    y_diff = abs(float(box1[2]) - float(box2[2]))
                    w_diff = abs(float(box1[3]) - float(box2[3]))
                    h_diff = abs(float(box1[4]) - float(box2[4]))
                    
                    # ì¤‘ë³µ íŒì • (ëª¨ë“  ì¢Œí‘œ ì°¨ì´ê°€ tolerance ë¯¸ë§Œ)
                    if (x_diff < tolerance and y_diff < tolerance and 
                        w_diff < tolerance and h_diff < tolerance):
                        
                        duplicate_pairs.append({
                            'kept_idx': i,
                            'removed_idx': j,
                            'differences': [x_diff, y_diff, w_diff, h_diff],
                            'class_id': cls1
                        })
                        removed_indices.add(j)  # ë‚˜ì¤‘ ì¸ë±ìŠ¤ ì œê±°
                        
            except (ValueError, IndexError, TypeError):
                continue
        
        # ì¤‘ë³µì´ ì•„ë‹Œ ë°•ìŠ¤ë§Œ ì¶”ê°€
        unique_boxes.append(box1)
    
    duplicate_info = {
        'removed_count': len(removed_indices),
        'duplicate_pairs': duplicate_pairs,
        'original_count': len(gt_boxes),
        'unique_count': len(unique_boxes)
    }
    
    return unique_boxes, duplicate_info


def improved_categorize_detection(precision, recall, conf_good, min_recall=0.8, min_precision=0.7):
    """
    ê°œì„ ëœ ê²€ì¶œ ì„±ëŠ¥ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í•¨ìˆ˜
    
    Args:
        precision: ì •ë°€ë„
        recall: ì¬í˜„ìœ¨ 
        conf_good: ì‹ ë¢°ë„ ì„ê³„ê°’ í†µê³¼ ì—¬ë¶€
        min_recall: ìµœì†Œ ì¬í˜„ìœ¨ ì„ê³„ê°’
        min_precision: ìµœì†Œ ì •ë°€ë„ ì„ê³„ê°’
    
    Returns:
        category: ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬ëª…
    """
    
    # 1. ìš°ìˆ˜í•œ ê²€ì¶œ (ë†’ì€ ì •ë°€ë„ + ë†’ì€ ì¬í˜„ìœ¨)
    if recall >= min_recall and precision >= min_precision:
        return 'good_detect' if conf_good else 'good_detect_low_conf'
    
    # 2. ë¶€ë¶„ì  ì„±ê³µ (ë§¤ìš° ë†’ì€ ì •ë°€ë„ + ì¤‘ê°„ ì¬í˜„ìœ¨)
    elif precision >= 0.9 and recall >= 0.5:
        return 'partial_detect' if conf_good else 'partial_detect_low_conf'
    
    # 3. ê²½ê³„ì„  ì„±ëŠ¥ (ì¤‘ê°„ ì •ë°€ë„ + ì¤‘ê°„ ì¬í˜„ìœ¨)
    elif recall >= 0.4 and precision >= 0.6:
        return 'borderline_detect'
    
    # 4. ì‹¤ì œ ê²€ì¶œ ì‹¤íŒ¨ (ë‚®ì€ ì¬í˜„ìœ¨)
    elif recall < 0.3:
        return 'miss_detect'
    
    # 5. ê±°ì§“ ì–‘ì„± ë¬¸ì œ (ë‚®ì€ ì •ë°€ë„)
    elif precision < 0.5:
        return 'false_detect'
    
    # 6. ê¸°íƒ€ (ì£¼ë¡œ ì‹ ë¢°ë„ ë¬¸ì œ)
    else:
        return 'low_conf'


# ğŸ›  ì¢Œí‘œ ë³€í™˜ ë¬¸ì œ ìˆ˜ì •

# ğŸ›  coords_type ë³€ìˆ˜ ì„ ì–¸ ë¬¸ì œ ìˆ˜ì •

def evaluate_detection_integrated(pred, targets, img_path, names, conf_thres, iou_thres, 
                                img_shape, min_recall=0.8, classes=None, debug=False):
    """
    ğŸ¯ ì¢Œí‘œ ë³€í™˜ ë¬¸ì œë¥¼ í•´ê²°í•œ í†µí•© ê²€ì¶œ í‰ê°€ í•¨ìˆ˜ (ìˆ˜ì •ë¨)
    """
    
    try:
        # ì´ë¯¸ì§€ í¬ê¸°
        if img_shape and len(img_shape) >= 2:
            h, w = img_shape[0], img_shape[1]
        else:
            h, w = 640, 640
        
        if debug:
            print(f"\nğŸ” COORDINATE FIXED DEBUG: {Path(img_path).name}")
            print(f"   Image size: {w}x{h}")
        
        # ğŸ¯ coords_type ê¸°ë³¸ê°’ ì„¤ì • (ì¤‘ìš”!)
        coords_type = "unknown"
        
        # ğŸ¯ targets ìƒíƒœ ìë™ ê°ì§€ ë° ì˜¬ë°”ë¥¸ ì²˜ë¦¬
        gt_boxes = []
        if len(targets) > 0:
            # ì²« ë²ˆì§¸ targetìœ¼ë¡œ ì¢Œí‘œ ì‹œìŠ¤í…œ ê°ì§€
            first_target = targets[0]
            if len(first_target) >= 5:
                x_test, y_test = first_target[1], first_target[2]
                
                # ì¢Œí‘œ ì‹œìŠ¤í…œ ìë™ ê°ì§€
                if 0 <= x_test <= 1 and 0 <= y_test <= 1:
                    coords_type = "normalized"
                    if debug:
                        print(f"   ğŸ“ Detected NORMALIZED coordinates")
                elif 0 <= x_test <= w and 0 <= y_test <= h:
                    coords_type = "pixel"  
                    if debug:
                        print(f"   ğŸ“ Detected PIXEL coordinates")
                else:
                    coords_type = "unknown"
                    if debug:
                        print(f"   âš ï¸  Unknown coordinate system: x={x_test}, y={y_test}")
            
            for i, target in enumerate(targets):
                if len(target) >= 5:
                    if hasattr(target, 'cpu'):
                        gt_box = target.cpu().numpy()
                    else:
                        gt_box = np.array(target)
                    
                    cls_id = gt_box[0]
                    x_center, y_center, width, height = gt_box[1:5]
                    
                    # ğŸ¯ ì¢Œí‘œ ì‹œìŠ¤í…œì— ë”°ë¥¸ ì ì ˆí•œ ì²˜ë¦¬
                    if coords_type == "normalized":
                        # ì´ë¯¸ normalized â†’ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        normalized_gt = [cls_id, float(x_center), float(y_center), float(width), float(height)]
                    elif coords_type == "pixel":
                        # pixel â†’ normalized ë³€í™˜
                        norm_x = float(x_center) / w
                        norm_y = float(y_center) / h
                        norm_w = float(width) / w
                        norm_h = float(height) / h
                        normalized_gt = [cls_id, norm_x, norm_y, norm_w, norm_h]
                    else:
                        # ì•ˆì „í•œ ë³€í™˜ ì‹œë„
                        norm_x = float(x_center) / w if x_center > 1 else float(x_center)
                        norm_y = float(y_center) / h if y_center > 1 else float(y_center)
                        norm_w = float(width) / w if width > 1 else float(width)
                        norm_h = float(height) / h if height > 1 else float(height)
                        normalized_gt = [cls_id, norm_x, norm_y, norm_w, norm_h]
                    
                    gt_boxes.append(normalized_gt)
                    
                    # ğŸ” ìƒì„¸ ë””ë²„ê¹… ì •ë³´
                    if debug and i < 3:
                        print(f"   GT {i+1}: cls={int(cls_id)}")
                        print(f"      Original: ({x_center:.6f}, {y_center:.6f}, {width:.6f}, {height:.6f})")
                        print(f"      Normalized: ({normalized_gt[1]:.6f}, {normalized_gt[2]:.6f}, {normalized_gt[3]:.6f}, {normalized_gt[4]:.6f})")
                        
                        # ì¢Œí‘œ ë²”ìœ„ ê²€ì¦
                        if not (0 <= normalized_gt[1] <= 1 and 0 <= normalized_gt[2] <= 1):
                            print(f"      âš ï¸  WARNING: Normalized coordinates out of range!")
        else:
            if debug:
                print(f"   ğŸ“ No targets found - coords_type set to 'unknown'")

        # ğŸ¯ ì¤‘ë³µ ë¼ë²¨ ì œê±° (ê³ ì •ë°€ë„ tolerance ì‚¬ìš©)
        original_gt_count = len(gt_boxes)
        if gt_boxes:
            # ì¢Œí‘œ ì •ë°€ë„ë¥¼ ê³ ë ¤í•œ tolerance ì¡°ì •
            high_precision_tolerance = 0.001  # ë” ì‘ì€ tolerance ì‚¬ìš©
            unique_gt_boxes, duplicate_info = remove_duplicate_labels(gt_boxes, high_precision_tolerance)
            if debug and duplicate_info['removed_count'] > 0:
                print(f"   ğŸ”§ ê³ ì •ë°€ë„ ì¤‘ë³µ ì œê±°: {original_gt_count} â†’ {len(unique_gt_boxes)} GT")
                for dup in duplicate_info['duplicate_pairs']:
                    diffs = dup['differences']
                    print(f"      ì œê±°: GT{dup['removed_idx']} (GT{dup['kept_idx']}ì™€ ì¤‘ë³µ)")
                    print(f"         ì°¨ì´: x={diffs[0]:.6f}, y={diffs[1]:.6f}, w={diffs[2]:.6f}, h={diffs[3]:.6f}")
        else:
            unique_gt_boxes = gt_boxes
            duplicate_info = {'removed_count': 0, 'duplicate_pairs': []}
        
        # í´ë˜ìŠ¤ í•„í„°ë§ (unique GT ì‚¬ìš©)
        filtered_gt_boxes = []
        if classes is not None:
            for gt_idx, gt_box in enumerate(unique_gt_boxes):
                gt_cls = safe_class_conversion(gt_box[0])
                if gt_cls < len(names) and gt_cls in [int(cls) for cls in classes]:
                    filtered_gt_boxes.append((gt_idx, gt_box))
        else:
            for gt_idx, gt_box in enumerate(unique_gt_boxes):
                gt_cls = safe_class_conversion(gt_box[0])
                if gt_cls < len(names):
                    filtered_gt_boxes.append((gt_idx, gt_box))
        
        if debug:
            print(f"   GT: {original_gt_count} total â†’ {len(unique_gt_boxes)} unique â†’ {len(filtered_gt_boxes)} filtered")
        
        # âœ… Prediction ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼í•˜ì§€ë§Œ ê³ ì •ë°€ë„)
        pred_boxes = []
        filtered_det = []
        
        if pred is not None and len(pred) > 0:
            if debug:
                print(f"   Pred: {len(pred)} detections")
            
            for i, (*xyxy, conf, cls) in enumerate(pred):
                # PIXEL â†’ NORMALIZED ë³€í™˜ (ê³ ì •ë°€ë„)
                x1, y1, x2, y2 = xyxy
                x_center = ((float(x1) + float(x2)) / 2) / w
                y_center = ((float(y1) + float(y2)) / 2) / h
                width = (float(x2) - float(x1)) / w
                height = (float(y2) - float(y1)) / h
                
                # ë””ë²„ê¹…: ë³€í™˜ëœ ì¢Œí‘œ ë²”ìœ„ ì²´í¬
                if debug and i < 3:
                    print(f"   Pred {i+1}: cls={int(cls)}, conf={conf:.3f}")
                    print(f"      pixel=({x1:.1f},{y1:.1f},{x2:.1f},{y2:.1f})")
                    print(f"      norm=({x_center:.6f},{y_center:.6f},{width:.6f},{height:.6f})")
                    if not (0 <= x_center <= 1 and 0 <= y_center <= 1):
                        print(f"      âš ï¸  WARNING: Pred coordinates out of range!")
                
                # ì €ì¥í•  ì •ë³´ êµ¬ì„± (ê³ ì •ë°€ë„)
                pred_info = {
                    'pixel_coords': [float(x1), float(y1), float(x2), float(y2)],
                    'normalized_coords': [x_center, y_center, width, height],  # ê³ ì •ë°€ë„ ìœ ì§€
                    'confidence': float(conf),
                    'class_id': int(cls)
                }
                pred_boxes.append(pred_info)
            
            # í´ë˜ìŠ¤ í•„í„°ë§
            if classes is not None:
                for i, pred_info in enumerate(pred_boxes):
                    cls_id = pred_info['class_id']
                    if cls_id < len(names) and cls_id in [int(cls) for cls in classes]:
                        filtered_det.append((i, pred_info))
            else:
                for i, pred_info in enumerate(pred_boxes):
                    cls_id = pred_info['class_id']
                    if cls_id < len(names):
                        filtered_det.append((i, pred_info))
            
            if debug:
                print(f"   Filtered pred: {len(filtered_det)}")
        
        # ì‹ ë¢°ë„ ì²´í¬
        all_conf_good = True
        if filtered_det:
            all_conf_good = all(pred_info['confidence'] >= conf_thres for _, pred_info in filtered_det)
        
        # âœ… IoU ë§¤ì¹­ (ê³ ì •ë°€ë„ ì¢Œí‘œ ì‚¬ìš©)
        matched_gt = set()
        matched_pred = set()
        match_details = []
        
        for pred_idx, pred_info in filtered_det:
            pred_cls = pred_info['class_id']
            if pred_cls >= len(names):
                continue
            
            pred_bbox = pred_info['normalized_coords']
            
            best_iou = 0
            best_gt_idx = -1
            best_gt_original_idx = -1
            
            for orig_gt_idx, gt_box in filtered_gt_boxes:
                gt_cls = safe_class_conversion(gt_box[0])
                if gt_cls >= len(names) or pred_cls != gt_cls:
                    continue
                
                gt_bbox = gt_box[1:5]
                iou = calculate_bbox_iou(pred_bbox, gt_bbox)
                
                if iou > best_iou:
                    best_iou = iou
                    best_gt_idx = len(match_details)
                    best_gt_original_idx = orig_gt_idx
            
            if best_iou > iou_thres and best_gt_original_idx >= 0:
                matched_gt.add(best_gt_original_idx)
                matched_pred.add(pred_idx)
                
                match_details.append({
                    'pred_idx': pred_idx,
                    'gt_idx': best_gt_original_idx,
                    'iou': float(best_iou),
                    'pred_conf': pred_info['confidence'],
                    'class_id': pred_cls
                })
                
                # ë””ë²„ê¹…: ë§¤ì¹­ ì •ë³´ ì¶œë ¥
                if debug:
                    print(f"   âœ… HIGH-PRECISION MATCH: Pred {pred_idx} â†” GT {best_gt_original_idx}, IoU={best_iou:.4f}, conf={pred_info['confidence']:.3f}")
        
        # âœ… ë©”íŠ¸ë¦­ ê³„ì‚° (unique GT ê¸°ì¤€)
        filtered_gt_count = len(filtered_gt_boxes)
        filtered_pred_count = len(filtered_det)
        
        if filtered_pred_count > 0:
            precision = len(matched_pred) / filtered_pred_count
        else:
            precision = 1.0 if filtered_gt_count == 0 else 0.0
            
        if filtered_gt_count > 0:
            recall = len(matched_gt) / filtered_gt_count
        else:
            recall = 1.0 if filtered_pred_count == 0 else 0.0
        
        # ğŸ¯ ê°œì„ ëœ ì¹´í…Œê³ ë¦¬ ê²°ì •
        if filtered_gt_count > 0:
            if recall >= min_recall and precision >= 0.7:
                category = 'good_detect' if all_conf_good else 'good_detect_low_conf'
            elif precision >= 0.9 and recall >= 0.5:
                category = 'partial_detect' if all_conf_good else 'partial_detect_low_conf'
            elif recall >= 0.4 and precision >= 0.6:
                category = 'borderline_detect'
            elif recall < 0.3:
                category = 'miss_detect'
            elif precision < 0.5:
                category = 'false_detect'
            else:
                category = 'low_conf'
        else:
            category = 'false_detect' if filtered_pred_count > 0 else 'background'
        
        # ğŸ¯ ê³ ì •ë°€ë„ ë””ë²„ê¹… ì •ë³´
        if debug:
            print(f"   ğŸ“Š HIGH-PRECISION RESULT: {category}")
            print(f"      Precision: {precision:.4f}, Recall: {recall:.4f}")
            print(f"      Coordinate system: {coords_type}")
            print(f"      Duplicates: {duplicate_info['removed_count']} removed (tolerance: 0.001)")
            print(f"      Matched: GT={len(matched_gt)}/{filtered_gt_count}, Pred={len(matched_pred)}/{filtered_pred_count}")
        
        # ë§¤ì¹­ ì •ë³´ ë° ë©”íŠ¸ë¦­ ì •ë³´ êµ¬ì„±
        matched_info = {
            'matched_gt': list(matched_gt),
            'matched_pred': list(matched_pred),
            'match_details': match_details
        }
        
        metrics = {
            'precision': float(precision),
            'recall': float(recall),
            'gt_count': original_gt_count,
            'unique_gt_count': len(unique_gt_boxes),
            'pred_count': len(pred_boxes),
            'filtered_gt_count': filtered_gt_count,
            'filtered_pred_count': filtered_pred_count,
            'matched_gt_count': len(matched_gt),
            'matched_pred_count': len(matched_pred),
            'duplicate_removed_count': duplicate_info['removed_count'],
            'confidence_good': all_conf_good,
            'coordinate_system': coords_type,  # âœ… ì´ì œ í•­ìƒ ì„ ì–¸ë¨
            'category_reason': f"P={precision:.4f}, R={recall:.4f}, coord={coords_type}, dup_removed={duplicate_info['removed_count']}"
        }
        
        return DetectionResult(
            img_path=img_path,
            category=category,
            pred_info=pred_boxes,
            gt_info=unique_gt_boxes,
            metrics=metrics,
            matched_info=matched_info
        )
        
    except Exception as e:
        print(f"âŒ ERROR in coordinate-fixed evaluation for {img_path}: {e}")
        import traceback
        traceback.print_exc()
        
        metrics = {
            'precision': 0.0, 'recall': 0.0, 'gt_count': 0, 'pred_count': 0,
            'filtered_gt_count': 0, 'filtered_pred_count': 0,
            'matched_gt_count': 0, 'matched_pred_count': 0, 'error': str(e)
        }
        
        return DetectionResult(
            img_path=img_path,
            category='background',
            pred_info=[],
            gt_info=[],
            metrics=metrics
        )
def setup_logging(log_level=logging.INFO):
    """Set up logging configuration."""
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[logging.StreamHandler()]
    )
    return logging.getLogger(__name__)

def safe_class_conversion(value):
    try:
        return int(float(value))  # float()ë¡œ ë¨¼ì € ë³€í™˜ í›„ int()
    except (ValueError, TypeError):
        return 0  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
    
class DetectionResult:
    """JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ê²€ì¶œ ê²°ê³¼ ì €ì¥ í´ë˜ìŠ¤"""
    def __init__(self, img_path, category, pred_info, gt_info, metrics, matched_info=None):
        self.img_path = str(img_path)
        self.category = category
        self.pred_info = pred_info
        self.gt_info = gt_info
        self.metrics = metrics
        self.matched_info = matched_info or {'matched_gt': [], 'matched_pred': []}
        self.parent_name = self.get_parent_name()
    
    def get_parent_name(self):
        """ë¶€ëª¨ ë””ë ‰í† ë¦¬ ì´ë¦„ ì¶”ì¶œ"""
        try:
            path_obj = Path(self.img_path)
            path_parts = path_obj.parts
            
            if 'JPEGImages' in path_parts:
                jpeg_idx = path_parts.index('JPEGImages')
                if jpeg_idx > 0:
                    return path_parts[jpeg_idx - 1]
            elif 'valid' in path_parts:
                valid_idx = path_parts.index('valid')
                if valid_idx > 0:
                    return path_parts[valid_idx - 1]
            elif 'images' in path_parts:
                images_idx = path_parts.index('images')
                if images_idx > 0:
                    return path_parts[images_idx - 1]
            
            if len(path_parts) >= 3:
                return path_parts[-3]
            elif len(path_parts) >= 2:
                return path_parts[-2]
            else:
                return 'unknown'
        except:
            return 'unknown'
    
    def to_dict(self):
        """JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'img_path': self.img_path,
            'category': self.category,
            'pred_info': self.pred_info,
            'gt_info': self.gt_info,
            'metrics': self.metrics,
            'matched_info': self.matched_info,
            'parent_name': self.parent_name
        }


def calculate_bbox_iou(box1, box2):
    """Calculate IoU between two bounding boxes (normalized coordinates)."""
    try:
        def xywh_to_xyxy(box):
            x_center, y_center, width, height = box[:4]
            x1 = x_center - width / 2
            y1 = y_center - height / 2
            x2 = x_center + width / 2
            y2 = y_center + height / 2
            return [x1, y1, x2, y2]
        
        box1_xyxy = xywh_to_xyxy(box1)
        box2_xyxy = xywh_to_xyxy(box2)
        
        x1 = max(box1_xyxy[0], box2_xyxy[0])
        y1 = max(box1_xyxy[1], box2_xyxy[1])
        x2 = min(box1_xyxy[2], box2_xyxy[2])
        y2 = min(box1_xyxy[3], box2_xyxy[3])
        
        if x2 <= x1 or y2 <= y1:
            return 0.0
        
        intersection = (x2 - x1) * (y2 - y1)
        area1 = (box1_xyxy[2] - box1_xyxy[0]) * (box1_xyxy[3] - box1_xyxy[1])
        area2 = (box2_xyxy[2] - box2_xyxy[0]) * (box2_xyxy[3] - box2_xyxy[1])
        union = area1 + area2 - intersection
        
        if union <= 0:
            return 0.0
        
        return intersection / union
    except:
        return 0.0


# ğŸ›  evaluate_detection_integrated í•¨ìˆ˜ ì™„ì „ ìˆ˜ì •

# def evaluate_detection_integrated(pred, targets, img_path, names, conf_thres, iou_thres, 
#                                 img_shape, min_recall=0.8, classes=None, debug=False):
#     """
#     ğŸ¯ í†µí•© ì™„ì„± ë²„ì „: ì¤‘ë³µ ë¼ë²¨ ì œê±° + ì¢Œí‘œ ìˆ˜ì • + ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
#     """
    
#     try:
#         # ì´ë¯¸ì§€ í¬ê¸°
#         if img_shape and len(img_shape) >= 2:
#             h, w = img_shape[0], img_shape[1]
#         else:
#             h, w = 640, 640
        
#         if debug:
#             print(f"\nğŸ” FIXED DEBUG: {Path(img_path).name}")
#             print(f"   Image size: {w}x{h}")
        
#         # âœ… Ground Truth ì²˜ë¦¬ (targetsëŠ” pixel ì¢Œí‘œ â†’ normalizedë¡œ ë³€í™˜)
#         gt_boxes = []
#         if len(targets) > 0:
#             for target in targets:
#                 if len(target) >= 5:
#                     if hasattr(target, 'cpu'):
#                         gt_box = target.cpu().numpy()
#                     else:
#                         gt_box = np.array(target)
                    
#                     # pixel ì¢Œí‘œë¥¼ normalized ì¢Œí‘œë¡œ ë³€í™˜
#                     cls_id = gt_box[0]
#                     x_center_pixel, y_center_pixel, width_pixel, height_pixel = gt_box[1:5]
                    
#                     # normalized ë³€í™˜
#                     x_center = x_center_pixel / w
#                     y_center = y_center_pixel / h
#                     width = width_pixel / w
#                     height = height_pixel / h
                    
#                     normalized_gt = [cls_id, x_center, y_center, width, height]
#                     gt_boxes.append(normalized_gt)
                    
#                     # ë””ë²„ê¹…: GT ì¢Œí‘œ ë²”ìœ„ ì²´í¬
#                     if debug and len(gt_boxes) <= 3:
#                         print(f"   GT {len(gt_boxes)}: cls={int(cls_id)}")
#                         print(f"      pixel=({x_center_pixel:.1f},{y_center_pixel:.1f},{width_pixel:.1f},{height_pixel:.1f})")
#                         print(f"      norm=({x_center:.3f},{y_center:.3f},{width:.3f},{height:.3f})")
#                         if not (0 <= x_center <= 1 and 0 <= y_center <= 1):
#                             print(f"   âš ï¸  WARNING: GT coordinates out of normalized range!")

#         # ğŸ¯ ì¤‘ë³µ ë¼ë²¨ ì œê±° (ëª¨ë“  GT ìˆ˜ì§‘ í›„ í•œ ë²ˆë§Œ ì‹¤í–‰)
#         original_gt_count = len(gt_boxes)
#         if gt_boxes:
#             unique_gt_boxes, duplicate_info = remove_duplicate_labels(gt_boxes, 0.01)
#             if debug and duplicate_info['removed_count'] > 0:
#                 print(f"   ğŸ”§ ì¤‘ë³µ ì œê±°: {original_gt_count} â†’ {len(unique_gt_boxes)} GT")
#                 for dup in duplicate_info['duplicate_pairs']:
#                     print(f"      ì œê±°: GT{dup['removed_idx']} (GT{dup['kept_idx']}ì™€ ì¤‘ë³µ)")
#         else:
#             unique_gt_boxes = gt_boxes
#             duplicate_info = {'removed_count': 0, 'duplicate_pairs': []}
        
#         # í´ë˜ìŠ¤ í•„í„°ë§ëœ GT ë°•ìŠ¤ (unique_gt_boxes ì‚¬ìš©)
#         filtered_gt_boxes = []
#         if classes is not None:
#             for gt_idx, gt_box in enumerate(unique_gt_boxes):
#                 gt_cls = safe_class_conversion(gt_box[0])
#                 if gt_cls < len(names) and gt_cls in [int(cls) for cls in classes]:
#                     filtered_gt_boxes.append((gt_idx, gt_box))
#         else:
#             for gt_idx, gt_box in enumerate(unique_gt_boxes):
#                 gt_cls = safe_class_conversion(gt_box[0])
#                 if gt_cls < len(names):
#                     filtered_gt_boxes.append((gt_idx, gt_box))
        
#         if debug:
#             print(f"   GT: {original_gt_count} total â†’ {len(unique_gt_boxes)} unique â†’ {len(filtered_gt_boxes)} filtered")
        
#         # âœ… Prediction ì²˜ë¦¬ (PIXEL ì¢Œí‘œ â†’ NORMALIZED ë³€í™˜)
#         pred_boxes = []
#         filtered_det = []
        
#         if pred is not None and len(pred) > 0:
#             if debug:
#                 print(f"   Pred: {len(pred)} detections")
            
#             for i, (*xyxy, conf, cls) in enumerate(pred):
#                 # PIXEL ì¢Œí‘œë¥¼ NORMALIZED ì¢Œí‘œë¡œ ë³€í™˜
#                 x1, y1, x2, y2 = xyxy
#                 x_center = ((x1 + x2) / 2) / w
#                 y_center = ((y1 + y2) / 2) / h
#                 width = (x2 - x1) / w
#                 height = (y2 - y1) / h
                
#                 # ë””ë²„ê¹…: ë³€í™˜ëœ ì¢Œí‘œ ë²”ìœ„ ì²´í¬
#                 if debug and i < 3:
#                     print(f"   Pred {i+1}: cls={int(cls)}, conf={conf:.3f}")
#                     print(f"      pixel=({x1:.1f},{y1:.1f},{x2:.1f},{y2:.1f})")
#                     print(f"      norm=({x_center:.3f},{y_center:.3f},{width:.3f},{height:.3f})")
#                     if not (0 <= x_center <= 1 and 0 <= y_center <= 1):
#                         print(f"   âš ï¸  WARNING: Pred coordinates out of range!")
                
#                 # ì €ì¥í•  ì •ë³´ êµ¬ì„± (JSON ì§ë ¬í™” ì•ˆì „)
#                 pred_info = {
#                     'pixel_coords': [float(x1), float(y1), float(x2), float(y2)],
#                     'normalized_coords': [float(x_center), float(y_center), float(width), float(height)],
#                     'confidence': float(conf),
#                     'class_id': int(cls)
#                 }
#                 pred_boxes.append(pred_info)
            
#             # í´ë˜ìŠ¤ í•„í„°ë§
#             if classes is not None:
#                 for i, pred_info in enumerate(pred_boxes):
#                     cls_id = pred_info['class_id']
#                     if cls_id < len(names) and cls_id in [int(cls) for cls in classes]:
#                         filtered_det.append((i, pred_info))
#             else:
#                 for i, pred_info in enumerate(pred_boxes):
#                     cls_id = pred_info['class_id']
#                     if cls_id < len(names):
#                         filtered_det.append((i, pred_info))
            
#             if debug:
#                 print(f"   Filtered pred: {len(filtered_det)}")
        
#         # ì‹ ë¢°ë„ ì²´í¬
#         all_conf_good = True
#         if filtered_det:
#             all_conf_good = all(pred_info['confidence'] >= conf_thres for _, pred_info in filtered_det)
        
#         # âœ… IoU ë§¤ì¹­ (ëª¨ë‘ NORMALIZED ì¢Œí‘œ ì‚¬ìš©)
#         matched_gt = set()
#         matched_pred = set()
#         match_details = []
        
#         for pred_idx, pred_info in filtered_det:
#             pred_cls = pred_info['class_id']
#             if pred_cls >= len(names):
#                 continue
            
#             pred_bbox = pred_info['normalized_coords']
            
#             best_iou = 0
#             best_gt_idx = -1
#             best_gt_original_idx = -1
            
#             for orig_gt_idx, gt_box in filtered_gt_boxes:
#                 gt_cls = safe_class_conversion(gt_box[0])
#                 if gt_cls >= len(names) or pred_cls != gt_cls:
#                     continue
                
#                 gt_bbox = gt_box[1:5]
#                 iou = calculate_bbox_iou(pred_bbox, gt_bbox)
                
#                 if iou > best_iou:
#                     best_iou = iou
#                     best_gt_idx = len(match_details)  # ë§¤ì¹˜ ë¦¬ìŠ¤íŠ¸ì—ì„œì˜ ì¸ë±ìŠ¤
#                     best_gt_original_idx = orig_gt_idx
            
#             if best_iou > iou_thres and best_gt_original_idx >= 0:
#                 matched_gt.add(best_gt_original_idx)
#                 matched_pred.add(pred_idx)
                
#                 match_details.append({
#                     'pred_idx': pred_idx,
#                     'gt_idx': best_gt_original_idx,
#                     'iou': float(best_iou),
#                     'pred_conf': pred_info['confidence'],
#                     'class_id': pred_cls
#                 })
                
#                 # ë””ë²„ê¹…: ë§¤ì¹­ ì •ë³´ ì¶œë ¥
#                 if debug:
#                     print(f"   âœ… MATCH: Pred {pred_idx} â†” GT {best_gt_original_idx}, IoU={best_iou:.3f}, conf={pred_info['confidence']:.3f}")
        
#         # âœ… ë©”íŠ¸ë¦­ ê³„ì‚° (unique GT ê¸°ì¤€)
#         filtered_gt_count = len(filtered_gt_boxes)
#         filtered_pred_count = len(filtered_det)
        
#         if filtered_pred_count > 0:
#             precision = len(matched_pred) / filtered_pred_count
#         else:
#             precision = 1.0 if filtered_gt_count == 0 else 0.0
            
#         if filtered_gt_count > 0:
#             recall = len(matched_gt) / filtered_gt_count
#         else:
#             recall = 1.0 if filtered_pred_count == 0 else 0.0
        
#         # ğŸ¯ ê°œì„ ëœ ì¹´í…Œê³ ë¦¬ ê²°ì •
#         if filtered_gt_count > 0:
#             # 1. ìš°ìˆ˜í•œ ê²€ì¶œ (ë†’ì€ ì •ë°€ë„ + ë†’ì€ ì¬í˜„ìœ¨)
#             if recall >= min_recall and precision >= 0.7:
#                 category = 'good_detect' if all_conf_good else 'good_detect_low_conf'
#             # 2. ë¶€ë¶„ì  ì„±ê³µ (ë§¤ìš° ë†’ì€ ì •ë°€ë„ + ì¤‘ê°„ ì¬í˜„ìœ¨)
#             elif precision >= 0.9 and recall >= 0.5:
#                 category = 'partial_detect' if all_conf_good else 'partial_detect_low_conf'
#             # 3. ê²½ê³„ì„  ì„±ëŠ¥ (ì¤‘ê°„ ì •ë°€ë„ + ì¤‘ê°„ ì¬í˜„ìœ¨)
#             elif recall >= 0.4 and precision >= 0.6:
#                 category = 'borderline_detect'
#             # 4. ì‹¤ì œ ê²€ì¶œ ì‹¤íŒ¨ (ë‚®ì€ ì¬í˜„ìœ¨)
#             elif recall < 0.3:
#                 category = 'miss_detect'
#             # 5. ê±°ì§“ ì–‘ì„± ë¬¸ì œ (ë‚®ì€ ì •ë°€ë„)
#             elif precision < 0.5:
#                 category = 'false_detect'
#             # 6. ê¸°íƒ€ (ì£¼ë¡œ ì‹ ë¢°ë„ ë¬¸ì œ)
#             else:
#                 category = 'low_conf'
#         else:
#             category = 'false_detect' if filtered_pred_count > 0 else 'background'
        
#         # ğŸ¯ ê°œì„ ëœ ë””ë²„ê¹… ì •ë³´
#         if debug:
#             print(f"   ğŸ“Š FIXED RESULT: {category}")
#             print(f"      Precision: {precision:.3f}, Recall: {recall:.3f}")
#             print(f"      Duplicates: {duplicate_info['removed_count']} removed")
#             print(f"      Original GT: {original_gt_count} â†’ Unique: {len(unique_gt_boxes)}")
#             print(f"      Matched: GT={len(matched_gt)}/{filtered_gt_count}, Pred={len(matched_pred)}/{filtered_pred_count}")
        
#         # ë§¤ì¹­ ì •ë³´ (JSON ì§ë ¬í™” ì•ˆì „)
#         matched_info = {
#             'matched_gt': list(matched_gt),
#             'matched_pred': list(matched_pred),
#             'match_details': match_details
#         }
        
#         # ğŸ¯ í™•ì¥ëœ ë©”íŠ¸ë¦­ ì •ë³´ (ì¤‘ë³µ ì •ë³´ í¬í•¨)
#         metrics = {
#             'precision': float(precision),
#             'recall': float(recall),
#             'gt_count': original_gt_count,  # ì›ë³¸ GT ìˆ˜
#             'unique_gt_count': len(unique_gt_boxes),  # ì¤‘ë³µ ì œê±° í›„ GT ìˆ˜
#             'pred_count': len(pred_boxes),
#             'filtered_gt_count': filtered_gt_count,
#             'filtered_pred_count': filtered_pred_count,
#             'matched_gt_count': len(matched_gt),
#             'matched_pred_count': len(matched_pred),
#             'duplicate_removed_count': duplicate_info['removed_count'],  # ì¤‘ë³µ ì œê±° ìˆ˜
#             'confidence_good': all_conf_good,
#             'category_reason': f"P={precision:.3f}, R={recall:.3f}, dup_removed={duplicate_info['removed_count']}"
#         }
        
#         return DetectionResult(
#             img_path=img_path,
#             category=category,
#             pred_info=pred_boxes,
#             gt_info=unique_gt_boxes,  # ğŸ¯ ì¤‘ë³µ ì œê±°ëœ GT ì €ì¥
#             metrics=metrics,
#             matched_info=matched_info
#         )
        
#     except Exception as e:
#         print(f"âŒ ERROR in fixed evaluation for {img_path}: {e}")
#         # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜
#         metrics = {
#             'precision': 0.0, 'recall': 0.0, 'gt_count': 0, 'pred_count': 0,
#             'filtered_gt_count': 0, 'filtered_pred_count': 0,
#             'matched_gt_count': 0, 'matched_pred_count': 0, 'error': str(e)
#         }
        
#         return DetectionResult(
#             img_path=img_path,
#             category='background',
#             pred_info=[],
#             gt_info=[],
#             metrics=metrics
#         )


def create_visualization_integrated(result_data, names, conf_thres, iou_thres):
    """
    ğŸ¯ í†µí•© ì‹œê°í™” í•¨ìˆ˜ - ë§¤ì¹­ ì •ë³´ í¬í•¨
    """
    try:
        # ê²°ê³¼ ë°ì´í„° ì¶”ì¶œ
        if isinstance(result_data, dict):
            img_path = result_data['img_path']
            category = result_data['category']
            pred_info = result_data['pred_info']
            gt_info = result_data['gt_info']
            metrics = result_data['metrics']
            matched_info = result_data.get('matched_info', {'matched_gt': [], 'matched_pred': []})
        else:
            img_path = result_data.img_path
            category = result_data.category
            pred_info = result_data.pred_info
            gt_info = result_data.gt_info
            metrics = result_data.metrics
            matched_info = result_data.matched_info
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        img = cv2.imread(str(img_path))
        if img is None:
            img = np.zeros((640, 640, 3), dtype=np.uint8)
            cv2.putText(img, f"Image not found: {Path(img_path).name}", 
                       (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        h, w = img.shape[:2]
        vis_img = img.copy()
        
        # ìƒ‰ìƒ ì •ì˜
        gt_color = (0, 255, 0)       # ì´ˆë¡: GT (unmatched)
        pred_color = (0, 0, 255)     # ë¹¨ê°•: Prediction (unmatched)
        matched_color = (255, 0, 0)  # íŒŒë‘: ë§¤ì¹˜ë¨
        low_conf_color = (128, 128, 128)  # íšŒìƒ‰: ë‚®ì€ ì‹ ë¢°ë„
        
        matched_gt_indices = set(matched_info.get('matched_gt', []))
        matched_pred_indices = set(matched_info.get('matched_pred', []))
        
        # âœ… Ground Truth ë°•ìŠ¤ ê·¸ë¦¬ê¸° (NORMALIZED â†’ PIXEL)
        for gt_idx, gt_box in enumerate(gt_info):
            if len(gt_box) >= 5:
                cls_id = safe_class_conversion(gt_box[0])  # âœ… '0.0' â†’ 0
                if cls_id < len(names):
                    # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
                    x_center, y_center, width, height = gt_box[1:5]
                    x1 = int((x_center - width/2) * w)
                    y1 = int((y_center - height/2) * h)
                    x2 = int((x_center + width/2) * w)
                    y2 = int((y_center + height/2) * h)
                    
                    # ê²½ê³„ ì²´í¬
                    x1, y1 = max(0, x1), max(0, y1)
                    x2, y2 = min(w, x2), min(h, y2)
                    
                    # ë§¤ì¹­ ì—¬ë¶€ì— ë”°ë¥¸ ìƒ‰ìƒ ì„ íƒ
                    color = matched_color if gt_idx in matched_gt_indices else gt_color
                    thickness = 4 if gt_idx in matched_gt_indices else 3
                    
                    # GT ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                    cv2.rectangle(vis_img, (x1, y1), (x2, y2), color, thickness)
                    
                    # ë¼ë²¨ ì¶”ê°€
                    label = f"GT: {names.get(cls_id, f'class_{cls_id}')}"
                    if gt_idx in matched_gt_indices:
                        label += " (Matched)"
                    
                    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                    cv2.rectangle(vis_img, (x1, y1-35), (x1+label_size[0]+10, y1), color, -1)
                    cv2.putText(vis_img, label, (x1+5, y1-10), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # âœ… Prediction ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        for pred_idx, pred_box in enumerate(pred_info):
            if isinstance(pred_box, dict):
                # í”½ì…€ ì¢Œí‘œ ì§ì ‘ ì‚¬ìš©
                if 'pixel_coords' in pred_box:
                    x1, y1, x2, y2 = pred_box['pixel_coords']
                else:
                    # ì •ê·œí™”ëœ ì¢Œí‘œì—ì„œ ë³€í™˜
                    x_center, y_center, width, height = pred_box['normalized_coords']
                    x1 = int((x_center - width/2) * w)
                    y1 = int((y_center - height/2) * h)
                    x2 = int((x_center + width/2) * w)
                    y2 = int((y_center + height/2) * h)
                
                conf = pred_box['confidence']
                cls_id = pred_box['class_id']
            else:
                # ê¸°ì¡´ í˜•ì‹ í˜¸í™˜ì„±
                if len(pred_box) >= 6:
                    x1, y1, x2, y2, conf, cls_id = pred_box[:6]
                    cls_id = safe_class_conversion(gt_box[0])
                else:
                    continue
            
            if cls_id < len(names):
                # ë§¤ì¹­ ì—¬ë¶€ì™€ ì‹ ë¢°ë„ì— ë”°ë¥¸ ìƒ‰ìƒ ì„ íƒ
                if pred_idx in matched_pred_indices:
                    color = matched_color
                    thickness = 4
                elif conf >= conf_thres:
                    color = pred_color
                    thickness = 3
                else:
                    color = low_conf_color
                    thickness = 2
                
                # ê²½ê³„ ì²´í¬
                x1, y1 = max(0, int(x1)), max(0, int(y1))
                x2, y2 = min(w, int(x2)), min(h, int(y2))
                
                # Prediction ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                cv2.rectangle(vis_img, (x1, y1), (x2, y2), color, thickness)
                
                # ë¼ë²¨ ì¶”ê°€
                label = f"Pred: {names.get(cls_id, f'class_{cls_id}')} {conf:.2f}"
                if pred_idx in matched_pred_indices:
                    label += " (Matched)"
                
                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                cv2.rectangle(vis_img, (x1, y2), (x1+label_size[0]+10, y2+35), color, -1)
                cv2.putText(vis_img, label, (x1+5, y2+25), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # ì •ë³´ íŒ¨ë„ ì¶”ê°€
        info_panel_height = 180
        info_panel = np.zeros((info_panel_height, w, 3), dtype=np.uint8)
        
        info_texts = [
            f"Category: {category}",
            f"Precision: {metrics.get('precision', 0):.3f}, Recall: {metrics.get('recall', 0):.3f}",
            f"GT Total: {metrics.get('gt_count', 0)}, Filtered: {metrics.get('filtered_gt_count', 0)}",
            f"Pred Total: {metrics.get('pred_count', 0)}, Filtered: {metrics.get('filtered_pred_count', 0)}",
            f"Matched GT: {metrics.get('matched_gt_count', 0)}, Matched Pred: {metrics.get('matched_pred_count', 0)}",
            f"Confidence Threshold: {conf_thres}, IoU Threshold: {iou_thres}",
            f"Image: {Path(img_path).name} ({w}x{h})"
        ]
        
        for i, text in enumerate(info_texts):
            cv2.putText(info_panel, text, (10, 25 + i*22), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # ë²”ë¡€ ì¶”ê°€
        legend_x = w - 300
        legend_y = 25
        
        cv2.rectangle(info_panel, (legend_x, legend_y), (legend_x+20, legend_y+15), gt_color, -1)
        cv2.putText(info_panel, "GT (Unmatched)", (legend_x+25, legend_y+12), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        cv2.rectangle(info_panel, (legend_x, legend_y+25), (legend_x+20, legend_y+40), pred_color, -1)
        cv2.putText(info_panel, "Pred (Unmatched)", (legend_x+25, legend_y+37), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        cv2.rectangle(info_panel, (legend_x, legend_y+50), (legend_x+20, legend_y+65), matched_color, -1)
        cv2.putText(info_panel, "Matched", (legend_x+25, legend_y+62), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        cv2.rectangle(info_panel, (legend_x, legend_y+75), (legend_x+20, legend_y+90), low_conf_color, -1)
        cv2.putText(info_panel, "Low Confidence", (legend_x+25, legend_y+87), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # ìµœì¢… ì´ë¯¸ì§€ í•©ì„±
        final_img = np.vstack([vis_img, info_panel])
        
        return final_img
        
    except Exception as e:
        print(f"Error creating integrated visualization: {e}")
        dummy_img = np.zeros((400, 800, 3), dtype=np.uint8)
        cv2.putText(dummy_img, f"Visualization Error: {str(e)}", (10, 200), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        return dummy_img


def post_process_integrated(results_file, save_dir, names, conf_thres, iou_thres, 
                          max_images_per_category=1000):
    """í†µí•© í›„ì²˜ë¦¬ í•¨ìˆ˜"""
    
    logger = setup_logging()
    
    try:
        # ê²°ê³¼ íŒŒì¼ ë¡œë“œ
        logger.info(f"Loading results from {results_file}...")
        
        with open(results_file, 'r') as f:
            all_results = [json.loads(line) for line in f]
        
        logger.info(f"Loaded {len(all_results)} results")
        
        # false/miss detectionë§Œ í•„í„°ë§
        problem_results = []
        for result in all_results:
            category = result.get('category', 'background')
            if category in ['false_detect', 'miss_detect', 'low_conf', 'partial_detect', 'borderline_detect']:  # ğŸ¯ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
                problem_results.append(result)
        
        logger.info(f"Found {len(problem_results)} problem detections")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
        categorized_results = defaultdict(list)
        for result in problem_results:
            category = result.get('category', 'background')
            categorized_results[category].append(result)
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ ì²˜ë¦¬
        for category, results in categorized_results.items():
            logger.info(f"Processing {category}: {len(results)} images")
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            category_dir = save_dir / 'categorized_results' / 'overall' / category
            category_dir.mkdir(parents=True, exist_ok=True)
            (category_dir / 'JPEGImages').mkdir(exist_ok=True)
            (category_dir / 'labels').mkdir(exist_ok=True)
            (category_dir / 'debug_images').mkdir(exist_ok=True)
            (category_dir / 'metadata').mkdir(exist_ok=True)
            
            # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
            if len(results) > max_images_per_category:
                results = random.sample(results, max_images_per_category)
                logger.info(f"Sampled {max_images_per_category} images from {category}")
            
            # ê° ì´ë¯¸ì§€ ì²˜ë¦¬
            for i, result in enumerate(tqdm(results, desc=f"Processing {category}")):
                try:
                    img_path = result['img_path']
                    gt_info = result['gt_info']
                    
                    img_name = Path(img_path).name
                    img_stem = Path(img_path).stem
                    
                    # 1. ì›ë³¸ ì´ë¯¸ì§€ ë³µì‚¬
                    if os.path.exists(img_path):
                        shutil.copy(img_path, category_dir / 'JPEGImages' / img_name)
                    
                    # 2. ë¼ë²¨ íŒŒì¼ ìƒì„± (normalized ì¢Œí‘œ)
                    if gt_info:
                        label_content = []
                        for gt_box in gt_info:
                            if len(gt_box) >= 5:
                                cls_id = int(float(gt_box[0]))
                                x, y, w, h = gt_box[1:5]
                                label_content.append(f"{cls_id} {x:.6f} {y:.6f} {w:.6f} {h:.6f}")
                        
                        if label_content:
                            label_path = category_dir / 'labels' / f"{img_stem}.txt"
                            with open(label_path, 'w') as f:
                                f.write('\n'.join(label_content))
                    
                    # 3. ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±
                    vis_img = create_visualization_integrated(result, names, conf_thres, iou_thres)
                    vis_path = category_dir / 'debug_images' / f"{img_stem}_analysis.jpg"
                    cv2.imwrite(str(vis_path), vis_img)
                    
                    # 4. ë©”íƒ€ë°ì´í„° ì €ì¥
                    metadata_path = category_dir / 'metadata' / f"{img_stem}_metadata.json"
                    with open(metadata_path, 'w') as f:
                        json.dump(result, f, indent=2)
                
                except Exception as e:
                    logger.warning(f"Error processing {img_path}: {e}")
                    continue
        
        logger.info("Integrated post-processing completed successfully")
        
        # ìš”ì•½ ì •ë³´ ìƒì„±
        generate_integrated_summary(categorized_results, save_dir)
        
    except Exception as e:
        logger.error(f"Error in integrated post-processing: {e}")
        raise


def generate_integrated_summary(categorized_results, save_dir):
    """í†µí•© ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
    logger = setup_logging()
    
    try:
        total_problems = sum(len(results) for results in categorized_results.values())
        
        # í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±
        summary_content = [
            "ğŸ¯ === INTEGRATED FALSE/MISS DETECTION ANALYSIS ===",
            f"ğŸ“Š Total problem detections: {total_problems}",
            f"ğŸ• Analysis completed: {Path().cwd()}",
            ""
        ]
        
        for category, results in categorized_results.items():
            count = len(results)
            percentage = (count / total_problems * 100) if total_problems > 0 else 0
            summary_content.append(f"ğŸ“‚ {category}: {count} ({percentage:.1f}%)")
        
        summary_content.extend([
            "",
            "ğŸ” === ANALYSIS LOCATIONS ===",
            "ğŸ“ Original images: categorized_results/overall/{category}/JPEGImages/",
            "ğŸ·ï¸  Labels: categorized_results/overall/{category}/labels/",
            "ğŸ¨ Visualizations: categorized_results/overall/{category}/debug_images/",
            "ğŸ“‹ Metadata: categorized_results/overall/{category}/metadata/",
            "",
            "ğŸ’¡ === RECOMMENDED ACTIONS ===",
            "ğŸ”´ 1. Review false_detect images for background patterns",
            "ğŸŸ¡ 2. Analyze miss_detect images for small objects or occlusions", 
            "ğŸŸ  3. Check low_conf images for threshold optimization",
            "ğŸ“ˆ 4. Focus on categories with highest percentages",
            "",
            "ğŸ¯ === NEXT STEPS ===",
            "â€¢ Use debug_images/ for visual pattern analysis",
            "â€¢ Check metadata/ for detailed metrics",
            "â€¢ Compare with baseline performance",
            "â€¢ Plan targeted data augmentation"
        ])
        
        summary_path = save_dir / 'integrated_analysis_summary.txt'
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(summary_content))
        
        logger.info(f"ğŸ“„ Integrated summary saved to {summary_path}")
        
        # JSON í†µê³„ ì €ì¥
        stats = {
            'total_problems': total_problems,
            'by_category': {cat: len(results) for cat, results in categorized_results.items()},
            'analysis_type': 'integrated_coordinate_fixed',
            'analysis_completed': True,
            'recommendations': {
                'false_detect': 'Add hard negative mining and background data',
                'miss_detect': 'Increase small object detection and reduce occlusion',
                'low_conf': 'Optimize confidence threshold and model training'
            }
        }
        
        stats_path = save_dir / 'integrated_statistics.json'
        with open(stats_path, 'w') as f:
            json.dump(stats, f, indent=2)
        
        logger.info(f"ğŸ“Š Integrated statistics saved to {stats_path}")
        
    except Exception as e:
        logger.error(f"Error generating integrated summary: {e}")


def test_final_integrated(data,
                        weights=None,
                        batch_size=32,
                        imgsz=640,
                        conf_thres=0.001,
                        iou_thres=0.6,
                        save_json=False,
                        single_cls=False,
                        augment=False,
                        verbose=False,
                        model=None,
                        dataloader=None,
                        save_dir=Path(''),
                        save_txt=False,
                        save_hybrid=False,
                        save_conf=False,
                        plots=True,
                        wandb_logger=None,
                        compute_loss=None,
                        half_precision=True,
                        trace=False,
                        is_coco=False,
                        v5_metric=False,
                        # Integrated parameters
                        enable_categorization=True,
                        min_recall=0.8,
                        categorization_classes=None,
                        device='',
                        task='val',
                        project='runs/test',
                        name='exp',
                        exist_ok=False,
                        max_problem_images=5000,
                        debug_first_images=10,
                        enable_post_processing=True):
    """
    ğŸ¯ ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
    - ì¢Œí‘œ ë³€í™˜ ë¬¸ì œ ì™„ì „ í•´ê²°
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  2ë‹¨ê³„ ì²˜ë¦¬
    - JSON ì§ë ¬í™” ì•ˆì „
    - ìƒì„¸í•œ ë§¤ì¹­ ì •ë³´ í¬í•¨
    """
    
    logger = setup_logging()
    
    # Initialize/load model and set device
    training = model is not None
    if training:
        device = next(model.parameters()).device
    else:
        set_logging()
        device = select_device(device, batch_size=batch_size)

        # Directories
        save_dir = Path(increment_path(Path(project) / name, exist_ok=exist_ok))
        (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)

        # Load model
        model = attempt_load(weights, map_location=device)
        gs = max(int(model.stride.max()), 32)
        imgsz = check_img_size(imgsz, s=gs)
        
        if trace:
            model = TracedModel(model, device, imgsz)

    # Half precision
    half = device.type != 'cpu' and half_precision
    if half:
        model.half()

    # Configure
    model.eval()
    if isinstance(data, str):
        is_coco = data.endswith('coco.yaml')
        with open(data) as f:
            data = yaml.load(f, Loader=yaml.SafeLoader)
    check_dataset(data)
    nc = 1 if single_cls else int(data['nc'])
    iouv = torch.linspace(0.5, 0.95, 10).to(device)
    niou = iouv.numel()

    # ê²°ê³¼ ìˆ˜ì§‘ ë³€ìˆ˜
    all_results = []
    category_stats = defaultdict(int)
    problem_count = 0
    
    # Create minimal opt object for dataloader
    class OptConfig:
        def __init__(self):
            self.rect = True
            self.cache_images = False
            self.image_weights = False
            self.quad = False
            self.prefix = ''
            self.workers = 8
            self.single_cls = single_cls
    
    opt_config = OptConfig()

    # Dataloader
    if not training:
        if device.type != 'cpu':
            model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))
        
        task = task if task in ('train', 'val', 'test') else 'val'
        dataloader = create_dataloader(data[task], imgsz, batch_size, gs, opt_config, pad=0.5, rect=True,
                                       prefix=colorstr(f'{task}: '))[0]

    if v5_metric:
        print("Testing with YOLOv5 AP metric...")
    
    seen = 0
    confusion_matrix = ConfusionMatrix(nc=nc)
    names = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}
    coco91class = coco80_to_coco91_class()
    s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')
    p, r, f1, mp, mr, map50, map, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.
    loss = torch.zeros(3, device=device)
    jdict, stats, ap, ap_class, wandb_images = [], [], [], [], []

    logger.info("ğŸ¯ Starting final integrated analysis...")

    for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):
        img = img.to(device, non_blocking=True)
        img = img.half() if half else img.float()
        img /= 255.0
        targets = targets.to(device)
        nb, _, height, width = img.shape

        with torch.no_grad():
            # Run model
            t = time_synchronized()
            out, train_out = model(img, augment=augment)
            t0 += time_synchronized() - t

            # Compute loss
            if compute_loss:
                loss += compute_loss([x.float() for x in train_out], targets)[1][:3]

            # Run NMS
            targets[:, 2:] *= torch.Tensor([width, height, width, height]).to(device)
            lb = [targets[targets[:, 0] == i, 1:] for i in range(nb)] if save_hybrid else []
            t = time_synchronized()
            out = non_max_suppression(out, conf_thres=conf_thres, iou_thres=iou_thres, labels=lb, multi_label=True)
            t1 += time_synchronized() - t

        # Statistics per image
        for si, pred in enumerate(out):
            labels = targets[targets[:, 0] == si, 1:]
            nl = len(labels)
            tcls = labels[:, 0].tolist() if nl else []
            path = Path(paths[si])
            seen += 1

            # ğŸ¯ Integrated categorization analysis
            if enable_categorization:
                try:
                    # ë””ë²„ê¹… ëª¨ë“œ ì„¤ì •
                    debug_mode = (seen <= debug_first_images)
                    
                    # í†µí•© í‰ê°€ í•¨ìˆ˜ í˜¸ì¶œ
                    result = evaluate_detection_integrated(
                        pred if pred is not None and len(pred) > 0 else torch.empty((0, 6)),
                        labels,  # targetsëŠ” ì´ë¯¸ pixel ì¢Œí‘œë¡œ ë³€í™˜ë¨
                        path, names, conf_thres, iou_thres, 
                        (height, width), min_recall, categorization_classes, debug_mode
                    )
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    category_stats[result.category] += 1
                    
                    # false/miss detectionë§Œ ì €ì¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
                    if result.category in ['false_detect', 'miss_detect', 'low_conf', 'partial_detect', 'borderline_detect']:  # ğŸ¯ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
                        all_results.append(result.to_dict())
                        problem_count += 1
                        
                        # ë©”ëª¨ë¦¬ ê´€ë¦¬
                        if problem_count > max_problem_images:
                            keep_ratio = 0.8
                            keep_count = int(len(all_results) * keep_ratio)
                            all_results = random.sample(all_results, keep_count)
                            logger.warning(f"Memory management: kept {keep_count} results")
                    
                    # ì£¼ê¸°ì  ë¡œê·¸
                    if seen % 1000 == 0:
                        logger.info(f"Processed {seen} images: Found {problem_count} problems")
                        current_stats = dict(category_stats)
                        logger.info(f"Current stats: {current_stats}")
                        
                except Exception as e:
                    if seen <= 10:  # ì´ˆê¸° ëª‡ ê°œë§Œ ë¡œê·¸
                        logger.warning(f"Error in categorization for {path}: {e}")

            # ì›ë˜ test.py ë¡œì§ ê³„ì†... (ê¸°ì¡´ê³¼ ë™ì¼)
            if len(pred) == 0:
                if nl:
                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))
                continue

            # Predictions
            predn = pred.clone()
            scale_coords(img[si].shape[1:], predn[:, :4], shapes[si][0], shapes[si][1])

            # Append to text file
            if save_txt:
                gn = torch.tensor(shapes[si][0])[[1, 0, 1, 0]]
                for *xyxy, conf, cls in predn.tolist():
                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()
                    line = (cls, *xywh, conf) if save_conf else (cls, *xywh)
                    with open(save_dir / 'labels' / (path.stem + '.txt'), 'a') as f:
                        f.write(('%g ' * len(line)).rstrip() % line + '\n')

            # Assign all predictions as incorrect
            correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)
            if nl:
                detected = []
                tcls_tensor = labels[:, 0]

                # target boxes
                tbox = xywh2xyxy(labels[:, 1:5])
                scale_coords(img[si].shape[1:], tbox, shapes[si][0], shapes[si][1])
                if plots:
                    confusion_matrix.process_batch(predn, torch.cat((labels[:, 0:1], tbox), 1))

                # Per target class
                for cls in torch.unique(tcls_tensor):
                    ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)
                    pi = (cls == pred[:, 5]).nonzero(as_tuple=False).view(-1)

                    if pi.shape[0]:
                        ious, i = box_iou(predn[pi, :4], tbox[ti]).max(1)

                        detected_set = set()
                        for j in (ious > iouv[0]).nonzero(as_tuple=False):
                            d = ti[i[j]]
                            if d.item() not in detected_set:
                                detected_set.add(d.item())
                                detected.append(d)
                                correct[pi[j]] = ious[j] > iouv
                                if len(detected) == nl:
                                    break

            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))

    # Compute statistics
    stats = [np.concatenate(x, 0) for x in zip(*stats)]
    if len(stats) and stats[0].any():
        p, r, ap, f1, ap_class = ap_per_class(*stats, plot=plots, v5_metric=v5_metric, save_dir=save_dir, names=names)
        ap50, ap = ap[:, 0], ap.mean(1)
        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()
        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)
    else:
        nt = torch.zeros(1)

    # Print results
    pf = '%20s' + '%12i' * 2 + '%12.3g' * 4
    print(pf % ('all', seen, nt.sum(), mp, mr, map50, map))

    if (verbose or (nc < 50 and not training)) and nc > 1 and len(stats):
        for i, c in enumerate(ap_class):
            print(pf % (names.get(c, f'class_{c}'), seen, nt[c], p[i], r[i], ap50[i], ap[i]))

    # Print collection results
    if enable_categorization:
        total_images = sum(category_stats.values())
        logger.info(f"\nğŸ¯ === FINAL INTEGRATED ANALYSIS COMPLETED ===")
        logger.info(f"ğŸ“Š Total images processed: {total_images}")
        logger.info(f"ğŸ” Problem detections collected: {len(all_results)}")
        
        for category, count in category_stats.items():
            percentage = (count / total_images * 100) if total_images > 0 else 0
            logger.info(f"ğŸ“‚ {category}: {count} ({percentage:.1f}%)")

    # ê²°ê³¼ ì €ì¥ ë° í›„ì²˜ë¦¬
    if enable_categorization and all_results:
        logger.info(f"\nğŸ¨ === STARTING INTEGRATED POST-PROCESSING ===")
        
        # ê²°ê³¼ ì €ì¥
        results_file = save_dir / 'integrated_problem_detections.jsonl'
        logger.info(f"ğŸ’¾ Saving {len(all_results)} problem detections to {results_file}")
        
        with open(results_file, 'w') as f:
            for result in all_results:
                f.write(json.dumps(result, default=str) + '\n')
        
        if enable_post_processing:
            # í›„ì²˜ë¦¬ ì‹¤í–‰
            try:
                post_process_integrated(
                    results_file=results_file,
                    save_dir=save_dir,
                    names=names,
                    conf_thres=conf_thres,
                    iou_thres=iou_thres,
                    max_images_per_category=1000
                )
                logger.info("ğŸ‰ Integrated post-processing completed successfully!")
                
            except Exception as e:
                logger.error(f"âŒ Error in post-processing: {e}")
                logger.info("âš ï¸  Basic test results are still available")

    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
    except:
        pass

    # Continue with plots and JSON saving
    if plots:
        confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))

    # Save JSON
    if save_json and len(jdict):
        w = Path(weights[0] if isinstance(weights, list) else weights).stem if weights is not None else ''
        anno_json = './coco/annotations/instances_val2017.json'
        pred_json = str(save_dir / f"{w}_predictions.json")
        print('\nEvaluating pycocotools mAP... saving %s...' % pred_json)
        with open(pred_json, 'w') as f:
            json.dump(jdict, f)

        try:
            from pycocotools.coco import COCO
            from pycocotools.cocoeval import COCOeval

            anno = COCO(anno_json)
            pred = anno.loadRes(pred_json)
            eval = COCOeval(anno, pred, 'bbox')
            if is_coco:
                eval.params.imgIds = [int(Path(x).stem) for x in dataloader.dataset.img_files]
            eval.evaluate()
            eval.accumulate()
            eval.summarize()
            map, map50 = eval.stats[:2]
        except Exception as e:
            print(f'pycocotools unable to run: {e}')

    # Print speeds
    t = tuple(x / seen * 1E3 for x in (t0, t1, t0 + t1)) + (imgsz, imgsz, batch_size)
    if not training:
        print('Speed: %.1f/%.1f/%.1f ms inference/NMS/total per %gx%g image at batch-size %g' % t)

    # Return results
    model.float()
    maps = np.zeros(nc) + map
    for i, c in enumerate(ap_class):
        maps[c] = ap[i]
    
    results = (mp, mr, map50, map, *(loss.cpu() / len(dataloader)).tolist())
    if enable_categorization:
        results = (*results, dict(category_stats))
    
    return results, maps, (t0, t1, t0 + t1)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(prog='test_final_integrated.py')
    parser.add_argument('--weights', nargs='+', type=str, default='yolov7.pt', help='model.pt path(s)')
    parser.add_argument('--data', type=str, default='data/coco.yaml', help='*.yaml path')
    parser.add_argument('--batch-size', type=int, default=32, help='size of each image batch')
    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')
    parser.add_argument('--conf-thres', type=float, default=0.001, help='object confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.65, help='IOU threshold for NMS')
    parser.add_argument('--task', default='val', help='train, val, test, speed or study')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--single-cls', action='store_true', help='treat as single-class dataset')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--verbose', action='store_true', help='report mAP by class')
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--save-hybrid', action='store_true', help='save label+prediction hybrid results to *.txt')
    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
    parser.add_argument('--save-json', action='store_true', help='save a cocoapi-compatible JSON results file')
    parser.add_argument('--project', default='runs/test', help='save to project/name')
    parser.add_argument('--name', default='exp', help='save to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--no-trace', action='store_true', help='don`t trace model')
    parser.add_argument('--v5-metric', action='store_true', help='assume maximum recall as 1.0 in AP calculation')
    
    # Integrated parameters
    parser.add_argument('--enable-categorization', action='store_true', default=True, help='enable problem detection collection')
    parser.add_argument('--min-recall', type=float, default=0.8, help='minimum recall for good detection')
    parser.add_argument('--categorization-classes', nargs='+', type=int, help='classes to analyze for categorization')
    parser.add_argument('--max-problem-images', type=int, default=5000, help='maximum problem images to collect')
    parser.add_argument('--debug-first-images', type=int, default=10, help='number of first images to debug')
    parser.add_argument('--enable-post-processing', action='store_true', default=True, help='enable post-processing')
    
    opt = parser.parse_args()
    opt.save_json |= opt.data.endswith('coco.yaml')
    opt.data = check_file(opt.data)
    print(opt)

    try:
        results, maps, times = test_final_integrated(
            opt.data,
            opt.weights,
            opt.batch_size,
            opt.img_size,
            opt.conf_thres,
            opt.iou_thres,
            opt.save_json,
            opt.single_cls,
            opt.augment,
            opt.verbose,
            save_txt=opt.save_txt | opt.save_hybrid,
            save_hybrid=opt.save_hybrid,
            save_conf=opt.save_conf,
            trace=not opt.no_trace,
            v5_metric=opt.v5_metric,
            enable_categorization=opt.enable_categorization,
            min_recall=opt.min_recall,
            categorization_classes=opt.categorization_classes,
            max_problem_images=opt.max_problem_images,
            debug_first_images=opt.debug_first_images,
            enable_post_processing=opt.enable_post_processing,
            device=opt.device,
            task=opt.task,
            project=opt.project,
            name=opt.name,
            exist_ok=opt.exist_ok
        )
        
        print(f"\nğŸ‰ Final integrated test completed successfully!")
        print(f"ğŸ“Š Results: mP={results[0]:.3f}, mR={results[1]:.3f}, mAP50={results[2]:.3f}, mAP={results[3]:.3f}")
        if len(results) > 7:
            print(f"ğŸ¯ Integrated coordinate-fixed analysis completed")
            
    except KeyboardInterrupt:
        print("\nâš ï¸  Process interrupted by user (Ctrl+C)")
        
    except Exception as e:
        print(f"\nâŒ Error occurred: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        print("âœ… Process completed and GPU memory cleaned up")