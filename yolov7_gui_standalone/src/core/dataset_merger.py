"""
ë°ì´í„°ì…‹ ë³‘í•© ëª¨ë“ˆ
ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ë¡œ í†µí•©í•˜ëŠ” ê¸°ëŠ¥
"""

import sys
import io
import os
import shutil
import yaml
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from collections import OrderedDict
import platform

# Windows ì½˜ì†” UTF-8 ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    try:
        if sys.version_info >= (3, 7):
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
            sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except Exception:
        pass


class DatasetMerger:
    """ì—¬ëŸ¬ YOLO ë°ì´í„°ì…‹ì„ í•˜ë‚˜ë¡œ ë³‘í•©í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, output_dir: Optional[Path] = None):
        """
        Args:
            output_dir: ë³‘í•©ëœ ë°ì´í„°ì…‹ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬
        """
        self.output_dir = output_dir or Path("merged_dataset")
        self.datasets = []
        self.merged_classes = OrderedDict()
        self.class_mapping = {}  # ê° ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ë§¤í•‘

    def add_dataset(self, dataset_path: Path, data_yaml_path: Optional[Path] = None):
        """
        ë°ì´í„°ì…‹ ì¶”ê°€

        Args:
            dataset_path: ë°ì´í„°ì…‹ ê²½ë¡œ
            data_yaml_path: data.yaml íŒŒì¼ ê²½ë¡œ (ì„ íƒ)
        """
        dataset_path = Path(dataset_path)

        if not dataset_path.exists():
            raise FileNotFoundError(f"ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {dataset_path}")

        # data.yaml ì°¾ê¸°
        if data_yaml_path is None:
            data_yaml_path = dataset_path / "data.yaml"
            if not data_yaml_path.exists():
                data_yaml_path = dataset_path / "dataset.yaml"

        # data.yaml ë¡œë“œ
        dataset_info = self._load_dataset_info(dataset_path, data_yaml_path)

        # ë°ì´í„°ì…‹ ì¶”ê°€
        self.datasets.append(dataset_info)

        # í´ë˜ìŠ¤ ë³‘í•©
        self._merge_classes(dataset_info)

        return dataset_info

    def _load_dataset_info(self, dataset_path: Path, data_yaml_path: Path) -> Dict[str, Any]:
        """ë°ì´í„°ì…‹ ì •ë³´ ë¡œë“œ"""
        info = {
            'path': dataset_path,
            'data_yaml': data_yaml_path,
            'classes': [],
            'nc': 0,
            'train_images': [],
            'valid_images': []
        }

        # data.yaml ì½ê¸°
        if data_yaml_path.exists():
            with open(data_yaml_path, 'r', encoding='utf-8') as f:
                data_config = yaml.safe_load(f)

            info['classes'] = data_config.get('names', [])
            info['nc'] = data_config.get('nc', len(info['classes']))
            info['original_config'] = data_config

        # train/valid ì´ë¯¸ì§€ ì°¾ê¸°
        info['train_images'] = self._find_images(dataset_path, 'train')
        info['valid_images'] = self._find_images(dataset_path, 'valid')

        return info

    def _find_images(self, dataset_path: Path, split: str) -> List[Path]:
        """ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì°¾ê¸°"""
        images = []

        # ì¼ë°˜ì ì¸ YOLO êµ¬ì¡° íƒìƒ‰
        search_paths = [
            dataset_path / 'images' / split,
            dataset_path / split / 'images',
            dataset_path / split,
            dataset_path / 'train' if split == 'train' else dataset_path / 'valid',
            dataset_path / 'val' if split == 'valid' else None
        ]

        search_paths = [p for p in search_paths if p is not None]

        for search_path in search_paths:
            if search_path.exists():
                for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
                    images.extend(list(search_path.glob(ext)))
                    images.extend(list(search_path.glob(ext.upper())))

        return images

    def _merge_classes(self, dataset_info: Dict[str, Any]):
        """í´ë˜ìŠ¤ ì •ë³´ ë³‘í•©"""
        dataset_classes = dataset_info['classes']
        dataset_idx = len(self.datasets) - 1

        # ê° ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ë¥¼ ì „ì—­ í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ì— ë§¤í•‘
        class_map = {}

        for local_idx, class_name in enumerate(dataset_classes):
            if class_name not in self.merged_classes:
                # ìƒˆë¡œìš´ í´ë˜ìŠ¤ ì¶”ê°€
                global_idx = len(self.merged_classes)
                self.merged_classes[class_name] = global_idx
            else:
                # ê¸°ì¡´ í´ë˜ìŠ¤ ì‚¬ìš©
                global_idx = self.merged_classes[class_name]

            class_map[local_idx] = global_idx

        self.class_mapping[dataset_idx] = class_map

    def merge(self, method: str = 'symlink', show_progress=None) -> Dict[str, Any]:
        """
        ë°ì´í„°ì…‹ ë³‘í•©

        Args:
            method: ë³‘í•© ë°©ì‹ ('symlink', 'list', 'copy')
            show_progress: ì§„í–‰ë¥  í‘œì‹œ ì½œë°± í•¨ìˆ˜

        Returns:
            ë³‘í•© ê²°ê³¼ ì •ë³´
        """
        if not self.datasets:
            raise ValueError("ë³‘í•©í•  ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤. add_dataset()ìœ¼ë¡œ ë¨¼ì € ì¶”ê°€í•˜ì„¸ìš”.")

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ë³‘í•© ë°©ì‹ì— ë”°ë¼ ì²˜ë¦¬
        if method == 'symlink':
            result = self._merge_symlink(show_progress)
        elif method == 'list':
            result = self._merge_list(show_progress)
        elif method == 'copy':
            result = self._merge_copy(show_progress)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë³‘í•© ë°©ì‹: {method}")

        # data.yaml ìƒì„±
        self._create_merged_yaml()

        return result

    def _merge_symlink(self, show_progress=None) -> Dict[str, Any]:
        """ì‹¬ë³¼ë¦­ ë§í¬ ë°©ì‹ìœ¼ë¡œ ë³‘í•©"""
        is_windows = platform.system() == 'Windows'

        if is_windows:
            print("âš ï¸ Windowsì—ì„œëŠ” symlinkê°€ ì œí•œì ì…ë‹ˆë‹¤. ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        # ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
        (self.output_dir / 'images' / 'train').mkdir(parents=True, exist_ok=True)
        (self.output_dir / 'images' / 'valid').mkdir(parents=True, exist_ok=True)
        (self.output_dir / 'labels' / 'train').mkdir(parents=True, exist_ok=True)
        (self.output_dir / 'labels' / 'valid').mkdir(parents=True, exist_ok=True)

        total_train = sum(len(ds['train_images']) for ds in self.datasets)
        total_valid = sum(len(ds['valid_images']) for ds in self.datasets)
        processed = 0
        total = total_train + total_valid

        for dataset_idx, dataset_info in enumerate(self.datasets):
            # Train ì´ë¯¸ì§€ ì²˜ë¦¬
            for img_path in dataset_info['train_images']:
                self._create_symlink_pair(
                    img_path,
                    self.output_dir / 'images' / 'train',
                    self.output_dir / 'labels' / 'train',
                    dataset_idx
                )
                processed += 1
                if show_progress and total > 0:
                    show_progress(processed / total * 100)

            # Valid ì´ë¯¸ì§€ ì²˜ë¦¬
            for img_path in dataset_info['valid_images']:
                self._create_symlink_pair(
                    img_path,
                    self.output_dir / 'images' / 'valid',
                    self.output_dir / 'labels' / 'valid',
                    dataset_idx
                )
                processed += 1
                if show_progress and total > 0:
                    show_progress(processed / total * 100)

        return {
            'method': 'symlink',
            'train_count': total_train,
            'valid_count': total_valid,
            'total': total
        }

    def _merge_list(self, show_progress=None) -> Dict[str, Any]:
        """ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ë°©ì‹ìœ¼ë¡œ ë³‘í•© (train.txt, valid.txt)"""
        train_list = []
        valid_list = []

        # ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± (ë¼ë²¨ë§Œ)
        (self.output_dir / 'labels' / 'train').mkdir(parents=True, exist_ok=True)
        (self.output_dir / 'labels' / 'valid').mkdir(parents=True, exist_ok=True)

        total_train = sum(len(ds['train_images']) for ds in self.datasets)
        total_valid = sum(len(ds['valid_images']) for ds in self.datasets)
        processed = 0
        total = total_train + total_valid

        for dataset_idx, dataset_info in enumerate(self.datasets):
            # Train ì´ë¯¸ì§€ ì²˜ë¦¬
            for img_path in dataset_info['train_images']:
                train_list.append(str(img_path.absolute()))
                # ë¼ë²¨ íŒŒì¼ ë³µì‚¬ (í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ë³€í™˜)
                self._copy_label_with_remap(
                    img_path,
                    self.output_dir / 'labels' / 'train',
                    dataset_idx
                )
                processed += 1
                if show_progress and total > 0:
                    show_progress(processed / total * 100)

            # Valid ì´ë¯¸ì§€ ì²˜ë¦¬
            for img_path in dataset_info['valid_images']:
                valid_list.append(str(img_path.absolute()))
                # ë¼ë²¨ íŒŒì¼ ë³µì‚¬ (í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ë³€í™˜)
                self._copy_label_with_remap(
                    img_path,
                    self.output_dir / 'labels' / 'valid',
                    dataset_idx
                )
                processed += 1
                if show_progress and total > 0:
                    show_progress(processed / total * 100)

        # train.txt, valid.txt ìƒì„±
        with open(self.output_dir / 'train.txt', 'w') as f:
            f.write('\n'.join(train_list))

        with open(self.output_dir / 'valid.txt', 'w') as f:
            f.write('\n'.join(valid_list))

        return {
            'method': 'list',
            'train_count': len(train_list),
            'valid_count': len(valid_list),
            'total': len(train_list) + len(valid_list)
        }

    def _merge_copy(self, show_progress=None) -> Dict[str, Any]:
        """íŒŒì¼ ë³µì‚¬ ë°©ì‹ìœ¼ë¡œ ë³‘í•©"""
        # ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
        (self.output_dir / 'images' / 'train').mkdir(parents=True, exist_ok=True)
        (self.output_dir / 'images' / 'valid').mkdir(parents=True, exist_ok=True)
        (self.output_dir / 'labels' / 'train').mkdir(parents=True, exist_ok=True)
        (self.output_dir / 'labels' / 'valid').mkdir(parents=True, exist_ok=True)

        total_train = sum(len(ds['train_images']) for ds in self.datasets)
        total_valid = sum(len(ds['valid_images']) for ds in self.datasets)
        processed = 0
        total = total_train + total_valid

        for dataset_idx, dataset_info in enumerate(self.datasets):
            # Train ì´ë¯¸ì§€ ì²˜ë¦¬
            for img_path in dataset_info['train_images']:
                self._copy_image_label_pair(
                    img_path,
                    self.output_dir / 'images' / 'train',
                    self.output_dir / 'labels' / 'train',
                    dataset_idx
                )
                processed += 1
                if show_progress and total > 0:
                    show_progress(processed / total * 100)

            # Valid ì´ë¯¸ì§€ ì²˜ë¦¬
            for img_path in dataset_info['valid_images']:
                self._copy_image_label_pair(
                    img_path,
                    self.output_dir / 'images' / 'valid',
                    self.output_dir / 'labels' / 'valid',
                    dataset_idx
                )
                processed += 1
                if show_progress and total > 0:
                    show_progress(processed / total * 100)

        return {
            'method': 'copy',
            'train_count': total_train,
            'valid_count': total_valid,
            'total': total
        }

    def _create_symlink_pair(self, img_path: Path, img_dest_dir: Path,
                            label_dest_dir: Path, dataset_idx: int):
        """ì´ë¯¸ì§€ì™€ ë¼ë²¨ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±"""
        # ê³ ìœ í•œ íŒŒì¼ëª… ìƒì„± (ì¶©ëŒ ë°©ì§€)
        unique_name = f"ds{dataset_idx}_{img_path.name}"

        # ì´ë¯¸ì§€ symlink
        img_link = img_dest_dir / unique_name
        if not img_link.exists():
            try:
                img_link.symlink_to(img_path.absolute())
            except OSError as e:
                # Windowsì—ì„œ ê¶Œí•œ ë¬¸ì œ ì‹œ ë³µì‚¬ë¡œ ëŒ€ì²´
                shutil.copy2(img_path, img_link)

        # ë¼ë²¨ íŒŒì¼ ì²˜ë¦¬ (í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ë³€í™˜ í•„ìš”)
        self._copy_label_with_remap(img_path, label_dest_dir, dataset_idx, unique_name)

    def _copy_label_with_remap(self, img_path: Path, label_dest_dir: Path,
                               dataset_idx: int, custom_name: Optional[str] = None):
        """ë¼ë²¨ íŒŒì¼ ë³µì‚¬í•˜ë©´ì„œ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ì¬ë§¤í•‘"""
        # ë¼ë²¨ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
        label_path = self._find_label_path(img_path)

        if label_path is None or not label_path.exists():
            return

        # ë¼ë²¨ íŒŒì¼ ì½ê¸°
        with open(label_path, 'r') as f:
            lines = f.readlines()

        # í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ì¬ë§¤í•‘
        class_map = self.class_mapping[dataset_idx]
        remapped_lines = []

        for line in lines:
            parts = line.strip().split()
            if len(parts) >= 5:  # class x y w h
                old_class_idx = int(parts[0])
                new_class_idx = class_map.get(old_class_idx, old_class_idx)
                remapped_lines.append(f"{new_class_idx} {' '.join(parts[1:])}\n")

        # ìƒˆ ë¼ë²¨ íŒŒì¼ ì €ì¥
        if custom_name:
            label_name = Path(custom_name).stem + '.txt'
        else:
            label_name = f"ds{dataset_idx}_{img_path.stem}.txt"

        label_dest = label_dest_dir / label_name

        with open(label_dest, 'w') as f:
            f.writelines(remapped_lines)

    def _copy_image_label_pair(self, img_path: Path, img_dest_dir: Path,
                               label_dest_dir: Path, dataset_idx: int):
        """ì´ë¯¸ì§€ì™€ ë¼ë²¨ íŒŒì¼ ë³µì‚¬"""
        # ê³ ìœ í•œ íŒŒì¼ëª… ìƒì„±
        unique_name = f"ds{dataset_idx}_{img_path.name}"

        # ì´ë¯¸ì§€ ë³µì‚¬
        img_dest = img_dest_dir / unique_name
        shutil.copy2(img_path, img_dest)

        # ë¼ë²¨ ë³µì‚¬ (í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ë³€í™˜)
        self._copy_label_with_remap(img_path, label_dest_dir, dataset_idx, unique_name)

    def _find_label_path(self, img_path: Path) -> Optional[Path]:
        """ì´ë¯¸ì§€ ê²½ë¡œë¡œë¶€í„° ë¼ë²¨ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°"""
        # ë¼ë²¨ ê²½ë¡œ í›„ë³´ë“¤
        possible_label_paths = [
            # images/train/xxx.jpg -> labels/train/xxx.txt
            Path(str(img_path).replace('/images/', '/labels/').replace('\\images\\', '\\labels\\')).with_suffix('.txt'),
            # train/xxx.jpg -> labels/train/xxx.txt
            img_path.parent.parent / 'labels' / img_path.parent.name / (img_path.stem + '.txt'),
            # train/images/xxx.jpg -> train/labels/xxx.txt
            img_path.parent.parent / 'labels' / (img_path.stem + '.txt'),
            # xxx.jpg -> xxx.txt (ê°™ì€ í´ë”)
            img_path.with_suffix('.txt')
        ]

        for label_path in possible_label_paths:
            if label_path.exists():
                return label_path

        return None

    def _create_merged_yaml(self):
        """ë³‘í•©ëœ data.yaml ìƒì„±"""
        data_yaml = {
            'path': str(self.output_dir.absolute()),
            'train': 'images/train' if (self.output_dir / 'images' / 'train').exists() else 'train.txt',
            'val': 'images/valid' if (self.output_dir / 'images' / 'valid').exists() else 'valid.txt',
            'nc': len(self.merged_classes),
            'names': list(self.merged_classes.keys())
        }

        yaml_path = self.output_dir / 'data.yaml'

        with open(yaml_path, 'w', encoding='utf-8') as f:
            yaml.dump(data_yaml, f, default_flow_style=False, allow_unicode=True, sort_keys=False)

        print(f"âœ… data.yaml ìƒì„± ì™„ë£Œ: {yaml_path}")
        return yaml_path

    def get_merge_summary(self) -> str:
        """ë³‘í•© ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        summary = []
        summary.append("=" * 50)
        summary.append("ğŸ“Š ë°ì´í„°ì…‹ ë³‘í•© ìš”ì•½")
        summary.append("=" * 50)
        summary.append(f"ì´ ë°ì´í„°ì…‹ ê°œìˆ˜: {len(self.datasets)}")
        summary.append(f"ë³‘í•©ëœ í´ë˜ìŠ¤ ê°œìˆ˜: {len(self.merged_classes)}")
        summary.append(f"í´ë˜ìŠ¤ ëª©ë¡: {', '.join(self.merged_classes.keys())}")
        summary.append("")

        total_train = sum(len(ds['train_images']) for ds in self.datasets)
        total_valid = sum(len(ds['valid_images']) for ds in self.datasets)

        summary.append(f"ì´ Train ì´ë¯¸ì§€: {total_train}")
        summary.append(f"ì´ Valid ì´ë¯¸ì§€: {total_valid}")
        summary.append(f"ì´ ì´ë¯¸ì§€: {total_train + total_valid}")
        summary.append("")

        summary.append("ë°ì´í„°ì…‹ë³„ ìƒì„¸:")
        for i, ds in enumerate(self.datasets):
            summary.append(f"  Dataset {i+1}: {ds['path'].name}")
            summary.append(f"    - Train: {len(ds['train_images'])}")
            summary.append(f"    - Valid: {len(ds['valid_images'])}")
            summary.append(f"    - Classes: {ds['nc']} ({', '.join(ds['classes'])})")

        summary.append("=" * 50)

        return '\n'.join(summary)


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("ğŸ§ª DatasetMerger í…ŒìŠ¤íŠ¸...")

    merger = DatasetMerger(Path("test_merged_dataset"))

    # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ê°€ìƒ ë°ì´í„°ì…‹ ì •ë³´ ìƒì„±
    print("âœ… DatasetMerger ì´ˆê¸°í™” ì„±ê³µ!")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {merger.output_dir}")
